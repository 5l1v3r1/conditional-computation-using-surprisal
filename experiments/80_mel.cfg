[experiment]
seed=0
folder=experiments/80_mel

[model]
num_tokens=1000
num_layers=2
num_hidden=512
num_mel_bins=80
big_model_dim=1000
tokenizer_training_text_path=librispeech-lm-norm.txt

[training]
base_path=/home/lugosch/data/LibriSpeech
lr=0.0005
lr_period=1
gamma=0.95
batch_size=32
num_epochs=100

[inference]
beam_width=10
